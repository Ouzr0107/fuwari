---
title: 课程分类模型技术记录
published: 2025-11-13
description: '为课上ymc的提问做准备'
image: ''
tags: ['技术', '课程']
category: '学习'
draft: false 
lang: ''
---

## N-Gram 模型

N-Gram 是一种基于统计语言模型的算法。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是 N 的字节片段序列。

每一个字节片段称为 gram，对所有 gram 的出现频度进行统计，并且按照事先设定好的阈值进行过滤，形成关键 gram 列表，也就是这个文本的向量特征空间，列表中的每一种 gram 就是一个特征向量维度。

该模型基于这样一种假设，第 N 个词的出现只与前面 N-1 个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计 N 个词同时出现的次数得到。常用的是二元的 Bi-Gram 和三元的 Tri-Gram。

### N-Gram模型用于评估语句是否合理

如果我们有一个由 m 个词组成的序列（或者说一个句子），我们希望算得概率，根据链式规则，可得

$$
p \{ w _ { 1 }, w _ { 2 }, \dots, w _ { m, 1 } \} = p \{ w _ { 1 } \} * p \{ w _ { 2 } | w _ { 1 } \} * p \{ w _ { 5 } | w _ { 1 }, w _ { 2 } \}, \dots, p \{ w _ { m } | w _ { 1 }, \dots, w _ { m - 1 } \}
$$

这个概率显然并不好算，不妨利用马尔科夫链

的假设，即当前这个词仅仅跟前面几个有限的词相关，因此也就不必追溯到最开始的那个词，这样便可以大幅缩减上述算式的长度。即

$$
p ( w _ { 1 }, w _ { 2 }, \dots, w _ { m } ) = p ( w _ { i } | w _ { i - n + 1, \dots, w _ { i - 1 } } )
$$

这个马尔科夫链的假设为什么好用？我想可能是在现实情况中，大家通过真实情况将n=1，2，3，....这些值都试过之后，得到的真实的效果和时间空间的开销权衡之后，发现能够使用。

## tf-idf

tf-idf（英语：term frequency–inverse document frequency）是一种用于信息检索与文本挖掘的常用加权技术。tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。tf-idf加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了tf-idf以外，互联网上的搜索引擎还会使用基于链接分析的评级方法，以确定文件在搜索结果中出现的顺序。

在一份给定的文件里，词频（term frequency，tf）指的是某一个给定的词语在该文件中出现的频率。这个数字是对词数（term count）的标准化，以防止它偏向长的文件。（同一个词语在长文件里可能会比短文件有更高的词数，而不管该词语重要与否。）对于在某一特定文件里的词语 $t_{i}$ 来说，它的重要性可表示为：

$$
{tf_{i,j}} ={\frac {n_{i,j}}{\sum _{k}n_{k,j}}}
$$

以上式子中假设文件 $d_{j}$ 中共有k个词语，$n_{k,j}$ 是 $t_{k}$ 在文件 $d_{j}$ 中出现的次数。因此，分子 $n_{i,j}$ 就是该词在文件 $d_{j}$ 中的出现次数，而分母则是在文件 $d_{j}$ 中所有字词的出现次数之和。

逆向文件频率（inverse document frequency，idf）是一个词语普遍重要性的度量。某一特定词语的 idf，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。通常，这个对数取以 10 为底（记为 $\lg$），如下所示：

$$
\mathrm{idf}_{i} = \lg \frac{|D|}{|\{j : t_{i} \in d_{j}\}|}
$$

然而，根据不同的应用场景和领域，也有使用其他类型的对数，例如以 2 为底的对数（提供信息量的度量，以比特为单位）或自然对数（常用于自然语言处理和其他计算应用中）。选择哪种对数底数取决于特定情境的需求：
其中

$|D|$：语料库中的文件总数

$|\{j : t_{i} \in d_{j}\}|$：包含词语 $t_{i}$ 的文件数目（即 $n_{i,j} \neq 0$ 的文件数目）

如果词语不在资料中，就导致分母为零，因此一般情况下使用 $1 + |{j : t_{i} \in d_{j}}|$

然后

$$
\mathrm{tfidf}_{i,j} = \mathrm{tf}_{i,j} \times \mathrm{idf}_{i}
$$

某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的 tf-idf。因此，tf-idf 倾向于过滤掉常见的词语，保留重要的词语。

## TextCNN

TextCNN（Convolutional Neural Networks for Sentence Classification）是由 Yoon Kim 在 2014 年提出的一种基于卷积神经网络（CNN）的文本分类模型。 该模型借鉴了图像处理领域的 CNN 技术，将其应用于自然语言处理（NLP）中的句子级分类任务，如情感分析、主题分类等。它证明了简单的 CNN 架构在文本数据上也能取得优异性能，尤其是在预训练词向量（如 Word2Vec）的辅助下。

### 1. 背景与动机

传统文本分类方法（如基于词袋模型的逻辑回归或 SVM）往往忽略词序和上下文信息，导致在捕捉句子语义时表现有限。Yoon Kim 受图像 CNN 的启发，将文本视为“图像”：句子中的词序列可以看作一维“信号”，通过卷积操作提取 n-gram 特征（局部模式）。 TextCNN 的核心思想是使用多个不同大小的卷积核来捕捉不同长度的短语模式，从而实现高效的特征提取。该模型在 2014 年 EMNLP 会议上发表，并在多个基准数据集上超越了当时的状态艺术（state-of-the-art）模型。

### 2. 模型架构

TextCNN 的架构相对简单，由输入层、卷积层、池化层和输出层组成。以下是其详细结构：

- **输入层（Embedding Layer）**：  
  输入是一个句子，由 m 个词组成，每个词用 d 维词向量表示（如使用预训练的 Google News Word2Vec，d=300）。这样，句子形成一个 m × d 的矩阵（类似于图像的 2D 矩阵）。如果句子长度不足 m，则用 padding 填充。 这允许模型处理变长句子。

- **卷积层（Convolution Layer）**：  
  使用多个一维卷积核（filter）对输入矩阵进行卷积操作。每个卷积核的宽度固定为 d（词向量维度），高度（region size）不同，例如 3、4、5（对应捕捉 3-gram、4-gram、5-gram 特征）。对于每个高度 h 的卷积核，有多个（例如 100 个）并行使用，以生成丰富的特征图（feature maps）。  
  数学上，对于输入矩阵 x 的位置 i 到 i+h-1 的子矩阵，卷积操作为：  

$$
c_i = f(w · x_{i:i+h-1} + b)
$$

  其中 w 是权重，b 是偏置，f 是激活函数（如 ReLU）。这会为每个卷积核生成一个长度为 m-h+1 的特征图。

- **池化层（Pooling Layer）**：  
  对每个特征图进行 1-max pooling，即从每个特征图中提取最大值。这能捕捉最重要的特征，并使特征向量固定长度（无论句子多长）。如果有 k 个不同大小的卷积核组，每个组有 n 个核，则 pooling 后得到 k × n 维向量。

- **输出层（Fully Connected Layer）**：  
  将所有 pooling 结果拼接成一个向量，传入 dropout 层（防止过拟合），然后通过全连接层和 softmax 进行分类。输出是类别的概率分布。

### 5. 优点与局限性

**优点**：

- **高效**：参数少，计算快，适合大规模数据。
- **捕捉局部特征**：有效提取 n-gram 模式，而无需复杂结构如 RNN。
- **易扩展**：可结合注意力机制（如 TextCNN with Attention）进一步提升。
- 在情感分析、问题分类等任务上表现优异，在 7 个基准数据集中的 4 个上达到 SOTA。

**局限性**：

- 难以捕捉长距离依赖（依赖于卷积核大小）。
- 对序列顺序敏感度不如 LSTM/Transformer。
- 在短文本分类中表现好，但长文本需修改。
